# app/app.py
"""
Mekong FNO Demo (Gradio app)

This module serves two purposes:
1) Provide Gradio UI callbacks for forecasting and evaluation.
2) Host a process-level singleton cache that loads the model + data once per process.

Key design constraints:
- This file may be run as a script: `python app/app.py`.
  Therefore we bootstrap `sys.path` early so absolute imports work regardless of CWD.
- Hugging Face Spaces typically provides a persistent `/data` volume. Runtime artifacts
  are routed to `RUNTIME_ROOT` (default: `/data/runtime` on HF, otherwise `<repo>/.runtime`).
- Some globals are initialized at import-time (e.g. `LAYOUT = get_layout()`).
  Keep import-time side effects minimal and deterministic.
"""

# =============================================================================
# Bootstrap import path (so `python app/app.py` works from any CWD)
# =============================================================================
# NOTE:
# - When running as a script, Python sets sys.path based on the working directory,
#   which can break absolute imports such as `from src.runner import ...`.
# - Add the repo root to sys.path to make imports stable.
import os, sys
ROOT = os.path.dirname(os.path.dirname(__file__))

# Ensure repo root is the very first import root.
try:
    while ROOT in sys.path:
        sys.path.remove(ROOT)
except Exception:
    pass
sys.path.insert(0, ROOT)

# =============================================================================
# Imports
# =============================================================================
# Standard library
import json, glob, io, time, threading
from pathlib import Path
from typing import Optional
from zoneinfo import ZoneInfo

# Third-party
import numpy as np
import pandas as pd
import tensorflow as tf

# Matplotlib: force non-interactive backend for server environments (HF/CI)
# - `MPLBACKEND` must be set *before* importing matplotlib in some setups.
# - `plt.ioff()` avoids accidental GUI usage.
os.environ.setdefault("MPLBACKEND", "Agg")
import matplotlib
matplotlib.use("Agg", force=True)
import matplotlib.pyplot as plt
plt.ioff()  # turn off interactive mode to avoid accidentally launching a GUI

import gradio as gr

# Project modules
from src.runner import TenYearUnifiedRunner, doy_sin_cos_series
from src.model_fno import SeasonalFNO1D
from src.runner import doy_no_leap

# Live daily means from MRC API (cached)
from src.live_mrc import get_recent_daily_cached
from src.backfill import read_backfill, write_backfill, series_from_any

# =============================================================================
# Global configuration / constants
# =============================================================================
# Model IO lengths: keep consistent with training
SEQ_LENGTH = 150
PRED_LENGTH = 7

# Repo root alias (historically named REPO_ROOT in this codebase)
REPO_ROOT = ROOT

# Input/asset locations (overridable via env)
ASSETS_DIR  = os.environ.get("ASSETS_DIR",  os.path.join(REPO_ROOT, "assets"))
WEIGHTS_DIR = os.environ.get("WEIGHTS_DIR", os.path.join(REPO_ROOT, "weights"))
CSV_DIR     = os.environ.get("CSV_DIR",     os.path.join(REPO_ROOT, "data"))

# Runtime root:
# - Local:  <repo>/.runtime
# - HF:     /data/runtime  (persistent volume)
DEFAULT_RUNTIME = "/data/runtime" if os.path.isdir("/data") else os.path.join(REPO_ROOT, ".runtime")
os.environ.setdefault("RUNTIME_ROOT", DEFAULT_RUNTIME)

# =============================================================================
# Runtime layout & canonical runtime files
# =============================================================================
# The runtime layout encapsulates all run-time outputs (caches, artifacts, logs).
# IMPORTANT: `LAYOUT = get_layout()` runs at import-time and may create directories.
from app.runtime_paths import get_layout
from app.runtime_files import (
    backfill_path,
    live_cache_daily_default,
    live_cache_3s,
    live_cache_pakse,
    assist_params_path,
    backtest_cache_path,
    backtest_metrics_cache_path,
)
from app.runtime_lock import lock_for_path, atomic_write_json, atomic_write_parquet

# NOTE: import-time side effect: may mkdir runtime folders.
LAYOUT = get_layout()

# Keep string versions for libraries that expect `str` paths (e.g. pandas read/write helpers).
RUNTIME_CACHE_DIR = str(LAYOUT.cache)
RUNTIME_ART_DIR   = str(LAYOUT.artifacts)

# Canonical runtime files (generated by runtime_files.py)
# - Prefer Path for locking/atomic writes
# - Provide str for legacy helpers/third-party APIs
BACKFILL_P: Path = backfill_path(LAYOUT)                  # Path for lock + atomic parquet writes
BACKFILL_PATH    = str(BACKFILL_P)                        # str for existing backfill helpers
LIVE_CACHE       = str(live_cache_daily_default(LAYOUT))  # JSON cache: target station recent daily series
LIVE_CACHE_3S    = str(live_cache_3s(LAYOUT))             # JSON cache: 3S upstream recent daily series
LIVE_CACHE_PAKSE = str(live_cache_pakse(LAYOUT))          # JSON cache: Pakse upstream recent daily series

# =============================================================================
# Station configuration
# =============================================================================
# Target station (Stung Treng): code used by MRC API
STATION_CODE = os.environ.get("STUNG_TRENG_CODE", "014501")

# Pakse (013901) CSV path & station code
PAKSE_CSV  = os.path.join(CSV_DIR, "Water Level.ManualLA_013901_Pakse.csv")
PAKSE_CODE = os.environ.get("PAKSE_CODE", "013901")

# 3S (014500) CSV path & station code
W3S_CSV  = os.path.join(CSV_DIR, "Water Level.TelemetryKH_014500_3S at Sekong bridge.csv")
W3S_CODE = os.environ.get("W3S_CODE", "014500")

# Assets used for inference / evaluation
CLIM_PATH = os.path.join(ASSETS_DIR, "clim_vec.npy")
NORM_PATH = os.path.join(ASSETS_DIR, "norm_stats.json")
PHASE_JSON = os.path.join(ASSETS_DIR, "phase_report.json")
RESID_PATH = os.path.join(ASSETS_DIR, "residual_sigma.json")  # historical residual band (fast uncertainty)

# Risk thresholds shown on plots (domain-specific)
ALARM_LEVEL = 10.7   # meters
FLOOD_LEVEL = 12.0   # meters

# Season configuration for upstream-assist behavior
WET_MONTHS = (6, 7, 8, 9, 10, 11)
DRY_SHRINK = float(os.environ.get("PAKSE_DRY_SHRINK", "0.4"))  # λ ∈ [0,1], shrink delta outside wet season

# =============================================================================
# Data utilities
# =============================================================================
def _merge_hist_and_live_no_gaps(water_daily_hist: pd.Series,
                                 live_daily: Optional[pd.Series],
                                 fill_small_holes: bool = True) -> pd.Series:
    """
    Merge two daily series with "live takes precedence" semantics, then reindex to a
    full daily calendar. Optionally fill only tiny internal gaps (≤ 1 day).

    Design intent:
    - Historical data provides long-term continuity.
    - Live/backfill data overwrites the tail (and may extend it).
    - A dense daily calendar is required for window-based forecasting.

    Args:
        water_daily_hist:
            Historical daily water level series (index: python date-like).
        live_daily:
            Optional "live/backfill" daily series to merge on top of history.
        fill_small_holes:
            If True, fill at most 1-day internal gaps (never extrapolate endpoints).

    Returns:
        pandas.Series:
            Continuous daily series (index: Python `date`, values: float meters).
    """
    def _to_dt_index(s: Optional[pd.Series]) -> pd.Series:
        if s is None or len(s) == 0:
            return pd.Series(dtype=float)
        idx = pd.to_datetime(list(s.index))
        return pd.Series(s.values, index=idx, dtype=float).sort_index()

    wd = _to_dt_index(water_daily_hist)
    if live_daily is not None and len(live_daily) > 0:
        live = _to_dt_index(live_daily)
        # For overlapping days, prefer live values (overwrite same-day entries).
        wd = pd.concat([wd, live]).groupby(level=0).last().sort_index()

    if len(wd) == 0:
        return pd.Series(dtype=float)

    # Reindex to a full daily calendar.
    full = pd.date_range(wd.index.min(), wd.index.max(), freq="D")
    wd = wd.reindex(full)

    # Fill only small internal gaps; keep endpoints untouched.
    if fill_small_holes:
        wd = wd.interpolate(limit=1, limit_area="inside")

    # Return index as Python date for compatibility with existing code paths.
    return pd.Series(wd.values, index=wd.index.date)

def _load_upstream_daily_csv(path: str) -> Optional[pd.Series]:
    """
    Load an upstream-station CSV and produce a daily-mean series.

    Behavior:
    - Parse timestamp column (best-effort matching).
    - Convert/localize to Asia/Bangkok (UTC+07).
    - Drop invalid timestamps.
    - Group by local calendar day -> daily mean.
    - Reindex to full daily calendar and fill tiny internal gaps (≤ 1 day).

    Args:
        path:
            Filesystem path to the station CSV.

    Returns:
        pandas.Series | None:
            Daily mean water level series (index: Python `date`, values: float),
            or None if the CSV doesn't exist / cannot be parsed meaningfully.
    """
    if not os.path.exists(path):
        print(f"[3S] file not found: {path}")
        return None

    df = pd.read_csv(path)
    cols = {c.lower(): c for c in df.columns}

    # Column matching: timestamp
    for k in ["timestamp (utc+07:00)", "timestamp", "ts", "datetime", "date", "time"]:
        if k in cols:
            tcol = cols[k]; break
    else:
        tcol = df.columns[0]

    # Column matching: numeric value
    for k in ["value", "h", "w", "water_level", "level"]:
        if k in cols:
            vcol = cols[k]; break
    else:
        num_cols = df.select_dtypes(include="number").columns
        if len(num_cols) == 0:
            print("[3S] no numeric column found")
            return None
        vcol = num_cols[0]

    raw = df[tcol].astype(str)

    # Detect whether timestamps include timezone info.
    has_tz_token = raw.str.contains(r'Z|[+-]\d{2}:\d{2}$', regex=True, na=False).any()
    if has_tz_token:
        ts = pd.to_datetime(raw, errors="coerce", utc=True).dt.tz_convert("Asia/Bangkok")
    else:
        ts = pd.to_datetime(raw, errors="coerce")
        ts = ts.dt.tz_localize("Asia/Bangkok")  # treat raw as already-local time

    df = df.loc[ts.notna()].copy()
    df["_ts_local"] = ts.dropna()
    # aggregate by local calendar days
    df["_date_local"] = df["_ts_local"].dt.date

    # Daily mean aggregation.
    s = df.groupby("_date_local")[vcol].mean().astype(float)
    # use python date as the index
    s.index = pd.Index(s.index, dtype="object")
    s = s.sort_index()

    print(f"[3S] daily series ready: len={len(s)}, range={min(s.index) if len(s) else None}→{max(s.index) if len(s) else None}")

    # Fill tiny internal gaps to prevent plot breaks / window failures.
    if len(s):
        full = pd.date_range(min(s.index), max(s.index), freq="D").date
        s = s.reindex(full)  # generate the complete daily calendar
        s = s.interpolate(limit=1, limit_area="inside")  # fill at most 1 day for internal gaps
        s = s.astype(float)

    return s

# -----------------------------------------------------------------------------
# Upstream-assist model fitting / application (shared by Tab1 & Tab2)
# -----------------------------------------------------------------------------
def _fit_3s_residual_model(
    df_backtest: pd.DataFrame,
    w3s_daily: pd.Series,
    k_grid=(0, 1, 2, 3, 4, 5),
    wet_months=None,
):
    """
    Fit a wet-season residual correction using upstream daily level and its first
    difference (Δlevel), scanning an integer lag `k` (days).

    Model form (standardized features):
        err ≈ a + b1 * z(level) + b2 * z(diff)

    Where:
    - err = h_true - h_pred  (residual on the target station)
    - upstream features are taken at (date - k)

    Notes:
    - Fit is restricted to wet-season months.
    - Uses ridge regression with small L2 for stability.

    Args:
        df_backtest:
            Backtest rows with columns: date, h_true, h_pred (and/or err).
        w3s_daily:
            Upstream station daily mean series.
        k_grid:
            Candidate lags to scan (days).
        wet_months:
            Iterable months treated as wet season (defaults to WET_MONTHS).

    Returns:
        dict | None:
            Best-fit parameter bundle, or None if insufficient samples.
    """
    if wet_months is None:
        wet_months = WET_MONTHS

    df = df_backtest.copy()
    df["date"] = pd.to_datetime(df["date"]).dt.date
    df["err"] = df["h_true"] - df["h_pred"]

    # Wet-season only.
    df = df[df["date"].map(lambda d: d.month in set(wet_months))].copy()

    if df.empty or w3s_daily is None or len(w3s_daily) == 0:
        return None

    w3s_lvl = w3s_daily
    w3s_d1  = w3s_daily.diff()

    best = None
    for k in k_grid:
        X_list, y_list = [], []
        for d, e in zip(df["date"], df["err"]):
            lag = d - pd.Timedelta(days=k)
            if (lag in w3s_lvl.index) and (lag in w3s_d1.index):
                x1 = w3s_lvl.get(lag)
                x2 = w3s_d1.get(lag)
                if pd.notna(e) and pd.notna(x1) and pd.notna(x2):
                    X_list.append([float(x1), float(x2)])
                    y_list.append(float(e))
        if len(X_list) < 40:
            continue

        X = np.asarray(X_list, np.float64)   # [N,2] -> [level, diff]
        y = np.asarray(y_list, np.float64)   # [N]

        # standardization + bias column
        mu = X.mean(axis=0); sd = X.std(axis=0) + 1e-8
        Xn = (X - mu) / sd
        Xd = np.c_[np.ones(len(Xn)), Xn]     # [1, z1, z2]

        # Ridge regression (small L2)
        lam = 1e-3
        A = Xd.T @ Xd + lam * np.eye(Xd.shape[1])
        coef = np.linalg.solve(A, Xd.T @ y)   # [a, b1, b2]

        yhat = Xd @ coef
        rmse_resid = float(np.sqrt(np.mean((y - yhat) ** 2)))

        cand = dict(
            a=float(coef[0]),
            b1=float(coef[1]),  # coefficient for the standardized level
            b2=float(coef[2]),  # coefficient for the standardized difference
            mu=mu.tolist(),
            sd=sd.tolist(),
            k=int(k),
            n=len(y),
            rmse_resid=rmse_resid,
            months=list(wet_months),
        )
        if (best is None) or (rmse_resid < best["rmse_resid"]):
            best = cand

    if best:
        print(f"[3S-fit] k={best['k']}, n={best['n']}, rmse_resid={best['rmse_resid']:.4f}, "
              f"coef a={best['a']:.4f}, b1={best['b1']:.4f}, b2={best['b2']:.4f}")
    else:
        print("[3S-fit] not enough samples to fit")
    return best

def _apply_3s_correction(df_backtest: pd.DataFrame, w3s_daily: pd.Series, params: dict):
    """
    Apply an upstream residual correction to a backtest dataframe.

    Policy:
    - Apply only during wet-season months.
    - Align upstream features by a learned lag `k` (days).
    - Allow interpolation over tiny internal gaps (≤ 1 day) after reindexing.
    - If upstream features are missing, apply delta=0 (keep baseline prediction).

    Returns:
      (y_corr, deltas):
        y_corr: corrected predictions aligned to df_backtest['date']
        deltas: additive correction applied to h_pred
    """
    if not params:
        return np.full(len(df_backtest), np.nan), np.full(len(df_backtest), np.nan)

    a  = float(params["a"])
    b1 = float(params["b1"]); b2 = float(params["b2"])
    mu = np.asarray(params["mu"], dtype=np.float64)   # [2] for [level, diff]
    sd = np.asarray(params["sd"], dtype=np.float64)   # [2]
    k  = int(params["k"])
    wet_months = set(params.get("months", list(WET_MONTHS)))

    # Align to the date index lagged by k days
    # Allowing interpolation over gaps up to 1 day
    dates = pd.to_datetime(df_backtest["date"]).dt.date
    lag_dates = [d - pd.Timedelta(days=k) for d in dates]

    # Reindex -> interpolate tiny gaps (≤ 1 day)
    s_lvl = w3s_daily.reindex(lag_dates)
    s_lvl = s_lvl.interpolate(limit=1, limit_area="inside").astype(float)

    # Take the first difference and set the first NaN to 0
    s_d1 = s_lvl.diff()
    if len(s_d1) > 0:
        first_valid = np.where(~pd.isna(s_d1))[0]
        if len(first_valid):
            s_d1.iloc[first_valid[0]] = 0.0

    y_corr = []; deltas = []
    for d, hp, x1, x2 in zip(dates, df_backtest["h_pred"], s_lvl.values, s_d1.values):
        if pd.isna(hp):
            y_corr.append(np.nan); deltas.append(np.nan); continue

        # Dry season: keep baseline.
        if d.month not in wet_months:
            y_corr.append(float(hp)); deltas.append(0.0); continue

        # Wet season: apply when upstream features exist.
        if pd.isna(x1) or pd.isna(x2):
            delta = 0.0
        else:
            z = (np.array([float(x1), float(x2)]) - mu) / (sd + 1e-8)
            delta = float(a + b1 * z[0] + b2 * z[1])

        y_corr.append(float(hp + delta))
        deltas.append(delta)

    return np.array(y_corr, np.float32), np.array(deltas, np.float32)

# -----------------------------------------------------------------------------
# Assist-param cache and backtest cache: concurrency-safe on-disk artifacts
# -----------------------------------------------------------------------------
def _load_or_fit_assist_params(
    station: str,
    upstream_name: str,
    upstream_daily: Optional[pd.Series],
    fit_fn,
    *,
    model_id: str,
    last_date,
):
    """
    Load (or fit) upstream-assist parameters with a concurrency-safe on-disk cache.

    Concurrency & consistency:
    - Fast path: unlocked read for performance.
    - Slow path: lock + double-check + fit + atomic write.
    - Atomic write prevents readers from seeing partially-written files.

    Cache key dimensions:
    - station, upstream_name, model_id, last_date

    Returns:
        params dict (truthy) or None if unavailable / insufficient data.
    """
    if upstream_daily is None or len(upstream_daily) == 0:
        return None

    p = assist_params_path(LAYOUT, station, upstream_name, model_id, last_date)

    # Fast path: unlocked read
    if p.exists():
        try:
            payload = json.loads(p.read_text(encoding="utf-8"))
            params = payload.get("params")
            if params:
                print(f"[assist] cache hit: {p}")
                return params
        except Exception:
            pass

    # Locked path: double-check + fit + atomic write
    with lock_for_path(p):
        if p.exists():
            try:
                payload = json.loads(p.read_text(encoding="utf-8"))
                params = payload.get("params")
                if params:
                    print(f"[assist] cache hit(after lock): {p}")
                    return params
            except Exception:
                pass

        print(f"[assist] cache miss -> fit: {p}")
        params = fit_fn()
        if params:
            payload = {
                "station": station,
                "upstream": upstream_name,
                "model_id": model_id,
                "last_date": str(last_date),
                "params": params,
            }
            atomic_write_json(p, payload)
            print(f"[assist] cached: {p}")
        return params

# =============================================================================
# Process-level singleton cache (model + data)
# =============================================================================
# These globals are created at import-time.
# - `_APP_LOCK` guards initialization to ensure only one thread builds the cache.
# - `_APP_CACHE` stores the ready-to-use service objects for all callbacks.
_APP_CACHE: dict = {}
_APP_LOCK = threading.Lock()

def _load_service(force_reload: bool = False):
    """
    Initialize and cache the end-to-end inference “service” (model, data, upstream series).

    Thread-safety:
    - Double-checked locking: unlocked fast path, then locked initialization path.
    - Build into `new_cache` and swap at the end to avoid exposing partial state.

    Args:
        force_reload:
            If True, rebuild the cache even if already ready.

    Returns:
        dict:
            A cache dict containing at least:
              - runner, water_daily, model_id, (optional) w3s_daily, pakse_daily, resid_sigma
    """
    # Fast path (unlocked)
    if _APP_CACHE.get("ready") and not force_reload:
        return _APP_CACHE

    # Locked path
    with _APP_LOCK:
        # double-check after acquiring the lock
        if _APP_CACHE.get("ready") and not force_reload:
            return _APP_CACHE

        t0 = time.perf_counter()

        # IMPORTANT:
        # - Do NOT clear the old cache first.
        # - Build `new_cache` fully, then atomically swap.
        new_cache: dict = {}

        runner = TenYearUnifiedRunner(CSV_DIR, seq_length=SEQ_LENGTH, pred_length=PRED_LENGTH)
        data = runner.load_range_data(2015, 2025, allow_missing_u=True)

        # Training-time artifacts: climatology + normalization stats
        runner.set_climatology(np.load(CLIM_PATH))
        st = json.load(open(NORM_PATH, "r", encoding="utf-8"))
        runner.norm_stats = st

        # Model build + weights load
        model = SeasonalFNO1D(modes=64, width=96, num_layers=4, input_features=6, dropout_rate=0.1, l2=1e-5)
        _ = model(np.zeros((1, SEQ_LENGTH, 6), dtype=np.float32), training=False)
        ckpt = _find_ckpt()
        model.load_weights(ckpt)
        runner.model = model

        model_id = Path(ckpt).name

        # ---------------------------------------------------------------------
        # Target station: build daily mean series from history + backfill + live
        # ---------------------------------------------------------------------
        df = pd.DataFrame(data, columns=['time_idx','x_pos','u','h','ts'])
        df['date'] = pd.to_datetime(df['ts']).dt.date
        water_daily_hist = df.groupby('date')['h'].mean()

        # read historical backfill (Parquet)
        backfill = read_backfill(BACKFILL_PATH)

        # fetch live daily means
        live_daily = None
        try:
            live_daily = get_recent_daily_cached(
                station_code=STATION_CODE,
                cache_path=LIVE_CACHE,
                ttl_seconds=900,
            )
        except Exception as e:
            print("[app] live fetch skipped:", e)

        live_daily = series_from_any(live_daily)
        backfill = series_from_any(backfill) if backfill is not None else None

        wd = _merge_hist_and_live_no_gaps(water_daily_hist, backfill, fill_small_holes=True)
        water_daily = _merge_hist_and_live_no_gaps(wd, live_daily, fill_small_holes=True)

        # Persist “stable” part of live data into backfill (<= today-1).
        # Rationale: avoid repeated API calls for old days; treat yesterday and earlier as stable.
        try:
            stable = None
            if live_daily is not None and len(live_daily) > 0:
                cutoff = (pd.Timestamp(_today_utc7_date()) - pd.Timedelta(days=1))

                stable_idx = [
                    d for d, v in live_daily.items()
                    if pd.notna(v) and pd.Timestamp(d) <= cutoff
                ]

                if len(stable_idx) > 0:
                    stable = pd.Series(
                        [live_daily[d] for d in stable_idx],
                        index=pd.Index(stable_idx, dtype="object"),
                        dtype=float
                    ).sort_index()

            if stable is not None and len(stable) > 0:
                # Lock on the canonical backfill parquet path to prevent concurrent writes.
                with lock_for_path(BACKFILL_P):
                    cur = series_from_any(read_backfill(BACKFILL_PATH))
                    cur = series_from_any(cur) if cur is not None else None

                    if cur is None or len(cur) == 0:
                        merged = stable
                    else:
                        merged = pd.concat([cur, stable]).groupby(level=0).last().sort_index()

                    _atomic_write_backfill_series(BACKFILL_P, merged)

        except Exception as e:
            print("[app] backfill write skipped:", e)

        print(f"[app] daily merged: days={len(water_daily)}, range={min(water_daily.index)}→{max(water_daily.index)}")

        # ---------------------------------------------------------------------
        # Upstream stations: 3S and Pakse (CSV history + live API tail)
        # ---------------------------------------------------------------------
        w3s_hist = _load_upstream_daily_csv(W3S_CSV)

        live_3s = None
        try:
            live_3s = get_recent_daily_cached(
                station_code=W3S_CODE,
                cache_path=LIVE_CACHE_3S,
                ttl_seconds=900,
            )
        except Exception as e:
            print("[3S] live fetch skipped:", e)

        live_3s = series_from_any(live_3s)

        if w3s_hist is not None and len(w3s_hist) > 0:
            w3s_daily = _merge_hist_and_live_no_gaps(w3s_hist, live_3s, fill_small_holes=True)
        else:
            w3s_daily = series_from_any(live_3s)

        print(f"[3S] path={W3S_CSV} exists={os.path.exists(W3S_CSV)}")
        if w3s_daily is None or len(w3s_daily) == 0:
            print("[3S] empty after load/merge")
        else:
            print(f"[3S] len={len(w3s_daily)}, range={min(w3s_daily.index)}→{max(w3s_daily.index)}")

        # Pakse daily series
        pakse_hist = _load_upstream_daily_csv(PAKSE_CSV)

        live_pakse = None
        try:
            live_pakse = get_recent_daily_cached(
                station_code=PAKSE_CODE,
                cache_path=LIVE_CACHE_PAKSE,
                ttl_seconds=900,
            )
        except Exception as e:
            print("[pakse] live fetch skipped:", e)

        live_pakse = series_from_any(live_pakse)

        if pakse_hist is not None and len(pakse_hist) > 0:
            pakse_daily = _merge_hist_and_live_no_gaps(pakse_hist, live_pakse, fill_small_holes=True)
        else:
            pakse_daily = series_from_any(live_pakse)

        if pakse_daily is not None and len(pakse_daily) > 0:
            print(f"[PAKSE] len={len(pakse_daily)}, range={min(pakse_daily.index)}→{max(pakse_daily.index)}")
        else:
            print("[PAKSE] empty after load/merge")

        # Historical residual band (fast uncertainty option)
        resid_sigma = None
        if os.path.exists(RESID_PATH):
            resid_sigma = json.load(open(RESID_PATH, "r", encoding="utf-8"))

        # Populate cache fully before swap
        new_cache.update(dict(
            runner=runner,
            water_daily=water_daily,
            resid_sigma=resid_sigma,
            mc_cache={},          # in-memory MC cache (keyed by (anchor, N))
            w3s_daily=w3s_daily,
            pakse_daily=pakse_daily,
            model_id=model_id,
            ready=True,
        ))

        # Debug: missing days in the recent tail (handled by interpolation/anchor selection)
        full = pd.date_range(min(water_daily.index), max(water_daily.index), freq="D")
        missing = set(full.date) - set(water_daily.index)
        tail_missing = sorted([d for d in missing if d >= (max(water_daily.index) - pd.Timedelta(days=14)).date()])
        if tail_missing:
            print(f"[app][warn] recent missing days auto-handled: {tail_missing}")

        print(f"[app] loaded in {time.perf_counter()-t0:.2f}s, days={len(water_daily)}")

        # Atomic swap: old cache remains valid for in-flight requests
        _APP_CACHE.clear()
        _APP_CACHE.update(new_cache)
        return _APP_CACHE

# =============================================================================
# Upstream-assist fitting helpers (Tab1 forecast overlays)
# =============================================================================
def _fit_pakse_params_for_tab1(runner, water_daily, pakse_daily, horizon_for_fit=1) -> Optional[dict]:
    """
    Fit a wet-season residual-correction model using upstream Pakse daily series.

    Intent:
    - Produce a parameter bundle compatible with `_apply_upstream_correction_future()`
      for Tab1's future-window overlay.

    Policy:
    - Fit is restricted to wet-season months (WET_MONTHS).
    - Lag k is selected by scanning a small integer grid and minimizing residual RMSE.
    - Fit data are derived from a k-day-ahead backtest window (constructed from history only).

    Returns:
        Optional[dict]:
            Parameter dictionary (same schema as `_fit_3s_residual_model`) or None if insufficient data.
    """
    if pakse_daily is None or len(pakse_daily) == 0:
        return None
    df_fit, _ = _backtest_ytd_1day(runner, water_daily, start="2025-01-01", horizon=horizon_for_fit)
    if df_fit is None or len(df_fit) == 0:
        return None
    params_pk = _fit_3s_residual_model(df_fit, pakse_daily, k_grid=(0, 1, 2, 3, 4, 5), wet_months=WET_MONTHS)
    return params_pk

def _fit_w3s_params_for_tab1(runner, water_daily, w3s_daily, horizon_for_fit=1):
    """
     Fit a wet-season residual-correction model using upstream 3S daily series.

     Intent:
     - Same as `_fit_pakse_params_for_tab1`, but uses the 3S station and a slightly smaller lag grid.

     Policy:
     - Wet-season only (WET_MONTHS).
     - Select k by scanning a small grid and minimizing residual RMSE.

     Returns:
         Optional[dict]:
             Parameter dictionary (same schema as `_fit_3s_residual_model`) or None if insufficient data.
     """
    if w3s_daily is None or len(w3s_daily) == 0:
        return None
    df_fit, _ = _backtest_ytd_1day(runner, water_daily, start="2025-01-01", horizon=horizon_for_fit)
    if df_fit is None or len(df_fit) == 0:
        return None
    params_3s = _fit_3s_residual_model(df_fit, w3s_daily, k_grid=(0, 1, 2, 3), wet_months=WET_MONTHS)
    return params_3s

def _apply_upstream_correction_future(y_pred_7, fut_dates, upstream_daily, params, shrink_dry=DRY_SHRINK, allow_interp=True):
    """
    Apply an upstream residual-correction model to a future forecast window.

    Intent:
    - Take a baseline future forecast (e.g., 7-day FNO) and produce an adjusted series
      using upstream level and Δlevel features at a learned lag k.

    Policy / guardrails:
    - Only use upstream values where lag_date <= today (UTC+07) to avoid “peeking” future upstream.
    - Optional interpolation fills internal gaps <= 1 day after aligning to lag dates.
    - Outside wet-season months, shrink the correction magnitude by `shrink_dry` ∈ [0, 1].

    Returns:
        (y_out, used, avail, k):
          y_out: float32 array (n,), corrected forecast; NaN where correction cannot be applied
                (NaN is intentional to break plot lines).
          used:  bool array (n,), True where a correction was actually applied.
          avail: int, number of usable corrected steps in this window.
          k:     int | None, lag from params, or None if params unavailable.
    """
    n = len(fut_dates)
    if not params or upstream_daily is None or len(upstream_daily) == 0:
        return np.full(n, np.nan, np.float32), np.zeros(n, dtype=bool), 0, None

    a  = float(params["a"])
    b1 = float(params["b1"]); b2 = float(params["b2"])
    mu = np.asarray(params["mu"], dtype=np.float64)
    sd = np.asarray(params["sd"], dtype=np.float64)
    k  = int(params["k"])
    wet_months = set(params.get("months", list(WET_MONTHS)))

    dates = pd.to_datetime(fut_dates).normalize().date
    lag_dates = np.array([d - pd.Timedelta(days=k) for d in dates], dtype="object")

    # Align upstream by lag; optionally fill tiny internal gaps to reduce “accidental NaN breaks”.
    s_lvl = upstream_daily.reindex(lag_dates)
    if allow_interp and len(s_lvl) > 0:
        s_lvl = s_lvl.interpolate(limit=1, limit_area="inside")

    # Disallow upstream values whose lag_date is in the future relative to UTC+07 “today”.
    today = _today_utc7_date()
    # Disable upstream values lag_date > today
    for i, ld in enumerate(lag_dates):
        if ld is None:
            s_lvl.iloc[i] = np.nan
            continue
        d_ld = pd.to_datetime(ld)
        if pd.isna(d_ld):
            s_lvl.iloc[i] = np.nan
            continue
        if d_ld.date() > today:
            s_lvl.iloc[i] = np.nan

    # Δlevel feature with a deterministic first value.
    s_d1 = s_lvl.diff()
    if len(s_d1) > 0:
        first_valid = np.where(~pd.isna(s_d1))[0]
        if len(first_valid):
            s_d1.iloc[first_valid[0]] = 0.0

    y_out  = np.array(y_pred_7, dtype=np.float32).copy()
    used   = np.zeros(n, dtype=bool)

    for i, (d, hp, x1, x2) in enumerate(zip(dates, y_pred_7, s_lvl.values, s_d1.values)):
        if pd.isna(hp) or pd.isna(x1) or pd.isna(x2):
            # Unavailable: set NaN (plot line break) rather than silently keeping baseline.
            y_out[i] = np.nan
            continue

        z = (np.array([float(x1), float(x2)]) - mu) / (sd + 1e-8)
        delta = float(a + b1 * z[0] + b2 * z[1])
        if d.month not in wet_months:
            delta *= float(shrink_dry)
        y_out[i] = float(hp + delta)
        used[i] = True

    avail = int(used.sum())
    return y_out, used, avail, k

# =============================================================================
# Anchor selection (robust window end-date selection)
# =============================================================================
def _latest_contiguous_anchor(water_daily: pd.Series, need: int = 120) -> pd.Timestamp.date:
    """
    Find the most recent contiguous run of valid daily observations and return the anchor date.

    Intent:
    - The model input window requires a dense daily calendar with no NaNs within the last `need` days.
      This helper chooses an anchor at the end of the latest gap-free run to avoid tail-end holes.

    Args:
        water_daily:
            Daily water level series (index: Python date, values: float).
        need:
            Required length (calendar days) of a contiguous, valid (non-NaN) block.

    Returns:
        datetime.date:
            Anchor date (end of the contiguous block) suitable for window building.

    Raises:
        ValueError:
            If there is no contiguous block of length >= `need`.
    """
    if len(water_daily) < need:
        raise ValueError(f"Not enough data for a {need}-day window (currently {len(water_daily)} days).")

    # Collect only dates with non-NaN values
    valid_dates = {d for d, v in water_daily.items() if pd.notna(v)}

    # Use Timestamp to step back day by day conveniently
    idx = pd.to_datetime(list(water_daily.index))
    d = idx.max().normalize()

    run = 0
    while d.date() >= idx.min().date():
        if d.date() in valid_dates:
            run += 1
            if run >= need:
                # We are scanning backwards: when run hits `need`, `d` is the start of the run.
                # Anchor is the end (most recent day) of that run.
                return (d + pd.Timedelta(days=need - 1)).date()
        else:
            run = 0
        d -= pd.Timedelta(days=1)

    raise ValueError(f"Not enough contiguous data for {need} days.")

# LIVE_CACHE   = os.path.join(RUNTIME_CACHE_DIR, "live_recent_daily.json")

def _find_ckpt(weights_dir=WEIGHTS_DIR):
    """
    Locate a TensorFlow checkpoint prefix under the given weights directory.

    Policy:
    - Prefer TensorFlow's latest_checkpoint() resolution when available.
    - Fall back to scanning "*.ckpt.index" (older layouts / incomplete metadata).

    Returns:
        str: Filesystem path to the checkpoint prefix (no extension).

    Raises:
        FileNotFoundError: If no checkpoint can be found in the directory.
    """
    ckpt = tf.train.latest_checkpoint(weights_dir)
    if ckpt: return ckpt
    idx = glob.glob(os.path.join(weights_dir, "*.ckpt.index"))
    if idx:  return idx[0].replace(".index","")
    raise FileNotFoundError("No TF checkpoint found in 'weights/'")

# =============================================================================
# Time utilities (UTC+07 calendar alignment)
# =============================================================================
def _today_utc7_date():
    """
    Return today's calendar date in UTC+07 (Asia/Bangkok), with a safe fallback.

    Rationale:
    - Station daily aggregation and “today” semantics are aligned to UTC+07.
    - Using a consistent day boundary avoids off-by-one issues around UTC midnight.

    Returns:
        datetime.date: Today's date in Asia/Bangkok when available; otherwise UTC date.
    """
    try:
        tz = ZoneInfo("Asia/Bangkok")
        return pd.Timestamp.now(tz=tz).date()
    except Exception:
        return pd.Timestamp.utcnow().date()

# =============================================================================
# Input normalization (training parity)
# =============================================================================
def _norm_inputs_like_train(X, st):
    """
    Normalize input features using the same statistics as training.

    Contract:
    - X has shape [batch, seq_len, n_features].
    - The feature channels at indices 0, 2, 3 correspond to:
        0: time index
        2: input water level
        3: Δ water level

    Returns:
        np.ndarray: Normalized copy of X.
    """
    Xn = X.copy()
    Xn[:, :, 0] = (Xn[:, :, 0] - st['t_mean']) / (st['t_std'] + 1e-8)
    Xn[:, :, 2] = (Xn[:, :, 2] - st['h_in_mean']) / (st['h_in_std'] + 1e-8)
    Xn[:, :, 3] = (Xn[:, :, 3] - st['dh_in_mean']) / (st['dh_in_std'] + 1e-8)
    return Xn

# =============================================================================
# Window builder (model input tensor assembly)
# =============================================================================
def _build_window_Xn(runner, water_daily: pd.Series, date_anchor: pd.Timestamp):
    """
    Construct the model input window ending at `date_anchor` and normalize it like training.

    Contract:
    - Window length is exactly `SEQ_LENGTH`.
    - Uses a no-leap calendar (Feb 29 is skipped) to match training conventions.
    - Any missing/NaN within the window is treated as a hard failure (caller surfaces UI error).

    Returns:
        (Xn, fut_dates):
          Xn: float32 array of shape (1, SEQ_LENGTH, 6), normalized.
          fut_dates: DatetimeIndex of length `PRED_LENGTH`, Feb 29 removed and padded if needed.
    """
    date_anchor = pd.to_datetime(date_anchor).normalize()
    L = int(SEQ_LENGTH)

    # Collect L days backward from the anchor (skip Feb 29 to match training calendar).
    days = []
    d = date_anchor
    while len(days) < L:
        if not (d.month == 2 and d.day == 29):
            days.append(d)
        d -= pd.Timedelta(days=1)
    days = days[::-1]  # ascending order

    # Feature channel 0: time index in a no-leap day count (relative to runner.train_years[0]).
    def _time_idx_for_date(dt: pd.Timestamp) -> int:
        base = pd.Timestamp(f"{runner.train_years[0]}-01-01")
        all_days = pd.date_range(base, dt, freq='D')
        all_days = all_days[~((all_days.month==2) & (all_days.day==29))]
        return len(all_days) - 1

    # Feature channels 2/3: h and Δh (hard fail if window contains missing).
    h_vals = []
    for dt in days:
        key = getattr(dt, "date", lambda: dt)()
        # If a date is missing from the index or its value is NaN, treat it as missing.
        if (key not in water_daily.index) or pd.isna(water_daily[key]):
            raise ValueError(
                f"Missing water level for {dt.date()} (NaN or absent), need continuous daily series with valid values."
            )
        h_vals.append(float(water_daily[key]))
    h_vals = np.asarray(h_vals, np.float32)
    dh1 = np.concatenate([[0.0], np.diff(h_vals)]).astype(np.float32)

    # Remaining channels: x_pos (constant 0), seasonal sin/cos.
    t_idx = np.asarray([_time_idx_for_date(dt) for dt in days], np.float32)
    x_pos = np.zeros_like(t_idx, np.float32)
    doy_sin, doy_cos = doy_sin_cos_series(days)

    feats6 = np.stack([t_idx, x_pos, h_vals, dh1, doy_sin, doy_cos], axis=1).astype(np.float32)

    # Normalize with training stats.
    st = runner.norm_stats
    Xn = feats6.copy()[None, :, :]
    Xn[:, :, 0] = (Xn[:, :, 0] - st['t_mean'])   / (st['t_std'] + 1e-8)
    Xn[:, :, 2] = (Xn[:, :, 2] - st['h_in_mean'])/ (st['h_in_std'] + 1e-8)
    Xn[:, :, 3] = (Xn[:, :, 3] - st['dh_in_mean'])/(st['dh_in_std'] + 1e-8)

    # Build future dates (no-leap). Pad if Feb 29 removal shortens the horizon.
    fut_dates = pd.date_range(date_anchor + pd.Timedelta(days=1), periods=PRED_LENGTH, freq='D')
    fut_dates = fut_dates[~((fut_dates.month==2) & (fut_dates.day==29))]
    while len(fut_dates) < PRED_LENGTH:
        fut_dates = fut_dates.append(fut_dates[-1:] + pd.Timedelta(days=1))
        fut_dates = fut_dates[~((fut_dates.month==2) & (fut_dates.day==29))]
    return Xn, fut_dates

# =============================================================================
# Inference core (forecast generation + uncertainty options)
# =============================================================================
def _predict_7_abs(runner, Xn, fut_dates, training=False):
    """
    Run the model forward to obtain absolute water levels for the future horizon.

    Model output semantics:
    - Model predicts standardized anomalies (z-scores) per horizon step.
    - We de-standardize with (h_std, h_mean) then add per-day climatology (no-leap DOY index).

    Args:
        training:
            If True, keeps dropout active for MC Dropout sampling.

    Returns:
        np.ndarray: float32 array shape (PRED_LENGTH,), absolute water levels in meters.
    """
    y_pred_n = runner.model(Xn, training=training).numpy()  # [1,7,1]
    st = runner.norm_stats
    y_pred_anom = (y_pred_n * st['h_std'] + st['h_mean'])[0, :, 0]  # [7]

    doys = [doy_no_leap(pd.to_datetime(d).normalize()) for d in fut_dates]
    clim_add = np.array([float(runner.clim[d]) for d in doys], dtype=np.float32)
    return (y_pred_anom + clim_add).astype(np.float32)

# =============================================================================
# Backtest core (k-day ahead YTD evaluation)
# =============================================================================
def _backtest_ytd_1day(runner, water_daily, start="2025-01-01", end=None, horizon=1):
    """
    Run a k-day-ahead backtest over a date range (default: 2025-01-01 → today, UTC+07).

    Evaluation protocol:
    - Prediction for day T uses only observations up to anchor day (T - k).
    - No recursive feeding of predicted values back into inputs.

    Returns:
        (df, rmse):
          df columns: date (target T), h_true, h_pred, err (h_pred - h_true)
          rmse: float | None
    """
    if end is None:
        end = _today_utc7_date()
    start = pd.to_datetime(start).date()
    end   = pd.to_datetime(end).date()

    k = int(horizon)
    if not (1 <= k <= PRED_LENGTH):
        raise ValueError(f"horizon must be in [1,{PRED_LENGTH}], got {horizon}")

    dates = pd.date_range(start, end, freq="D").date
    preds, trues, out_dates = [], [], []

    for T in dates:
        # No-leap parity with training/inference.
        if T.month == 2 and T.day == 29:
            continue
        anchor = pd.Timestamp(T) - pd.Timedelta(days=k)

        # Skip if anchor/target missing or NaN.
        if (anchor.date() not in water_daily.index) or (T not in water_daily.index):
            continue
        if pd.isna(water_daily[anchor.date()]) or pd.isna(water_daily[T]):
            continue
        try:
            Xn, fut_dates = _build_window_Xn(runner, water_daily, anchor)
        except Exception:
            continue

        y_abs = _predict_7_abs(runner, Xn, fut_dates, training=False)  # [7]
        predk = float(y_abs[k - 1])  # k-day ahead
        true  = float(water_daily[T])

        out_dates.append(pd.to_datetime(T))
        preds.append(predk)
        trues.append(true)

    df = pd.DataFrame({"date": out_dates, "h_true": trues, "h_pred": preds})
    if len(df):
        df["err"] = df["h_pred"] - df["h_true"]
        rmse = float(np.sqrt(np.mean(df["err"]**2)))
    else:
        rmse = None
    return df, rmse

# =============================================================================
# Tab2 callback: YTD backtest plot (Observed vs Predicted)
# =============================================================================
def ui_eval_ytd(horizon=1):
    """
    Tab2 callback: run a 2025 YTD k-day-ahead backtest and render an OvsP plot.

    Enhancements:
    - Optionally fits upstream assist models (3S, Pakse) on the same backtest rows
      and overlays corrected predictions (for comparison only).

    Returns:
        (fig, note, df):
          fig: matplotlib.figure.Figure | None
          note: summary string (date window, horizon, N, RMSE, overlays)
          df: columns include date, h_true, h_pred (+ optional assist columns)
    """
    S = _load_service()
    runner, water_daily = S["runner"], S["water_daily"]
    w3s_daily   = S.get("w3s_daily")
    pakse_daily = S.get("pakse_daily")

    year = 2025
    k = int(horizon)
    last_date = max(water_daily.index)  # python date
    model_id = S.get("model_id") or Path(_find_ckpt()).name

    df, rmse = _load_or_run_backtest_ytd_cached(
        year=year,
        k=k,
        station=STATION_CODE,
        model_id=model_id,
        last_date=last_date,
        runner=runner,
        water_daily=water_daily,
    )

    if df is None or len(df) == 0:
        return None, f"Not enough data to backtest 2025 YTD (h={horizon}).", pd.DataFrame()

    # -------------------------------------------------------------------------
    # Optional overlays: assist correction on the *same* backtest rows
    # -------------------------------------------------------------------------
    note_extra = ""

    y_corr = None
    if w3s_daily is not None and len(w3s_daily) > 0:
        params = _fit_3s_residual_model(df, w3s_daily, k_grid=(0, 1, 2, 3))
        if params:
            y_corr, deltas = _apply_3s_correction(df, w3s_daily, params)
            mask = ~np.isnan(y_corr) & ~np.isnan(df["h_true"].values)
            if mask.sum() >= 30:
                rmse_corr = float(np.sqrt(np.mean((y_corr[mask] - df["h_true"].values[mask]) ** 2)))
                note_extra += f" | 3S assist: k={params['k']}, N={int(mask.sum())}, RMSE_adj={rmse_corr:.3f} m (vs {rmse:.3f})"
            df["h_pred_3S"] = y_corr
            df["delta_3S"]  = deltas

    # ---- Pakse assist on this horizon ----
    y_corr_pk = None
    if pakse_daily is not None and len(pakse_daily) > 0:
        params_pk = _fit_3s_residual_model(df, pakse_daily, k_grid=(0, 1, 2, 3))
        if params_pk:
            y_corr_pk, deltas_pk = _apply_3s_correction(df, pakse_daily, params_pk)
            mask_pk = ~np.isnan(y_corr_pk) & ~np.isnan(df["h_true"].values)
            if mask_pk.sum() >= 30:
                rmse_pk = float(np.sqrt(np.mean((y_corr_pk[mask_pk] - df["h_true"].values[mask_pk]) ** 2)))
                note_extra += f" | Pakse assist: k={params_pk['k']}, N={int(mask_pk.sum())}, RMSE_adj={rmse_pk:.3f} m (vs {rmse:.3f})"
            df["h_pred_Pakse"] = y_corr_pk
            df["delta_Pakse"]  = deltas_pk

    # -------------------------------------------------------------------------
    # Plot
    # -------------------------------------------------------------------------
    fig = plt.figure(figsize=(10.5, 4.0))
    plt.plot(df["date"], df["h_true"], label="Observed", linewidth=1.8)
    plt.plot(df["date"], df["h_pred"], label=f"FNO ({horizon}-day ahead)", linewidth=1.8)
    if y_corr is not None:
        plt.plot(df["date"], y_corr, label=f"FNO + 3S ({horizon}-day)", linewidth=1.8)
    if y_corr_pk is not None:
        plt.plot(df["date"], y_corr_pk, label=f"FNO + Pakse ({horizon}-day)", linewidth=1.8)
    plt.axhline(ALARM_LEVEL, linestyle="--", color="darkgoldenrod", linewidth=1, label=f"Alarm {ALARM_LEVEL:.1f} m")
    plt.axhline(FLOOD_LEVEL, linestyle="--", color="red", linewidth=1, label=f"Flood {FLOOD_LEVEL:.1f} m")
    plt.title(f"2025 YTD — Observed vs Predicted ({horizon}-day ahead)")
    plt.xlabel("Date"); plt.ylabel("Water Level (m)")
    plt.xticks(rotation=20); plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout()

    note = (
        f"Backtest window: 2025-01-01 → {df['date'].iloc[-1].date()} "
        f"| Horizon={horizon} day(s) | N={len(df)} | RMSE={rmse:.3f} m "
        f"| Alarm={ALARM_LEVEL:.1f} m, Flood={FLOOD_LEVEL:.1f} m"
        f"{note_extra}"
    )

    cols = ["date", "h_true", "h_pred"]
    if "h_pred_3S" in df.columns:
        if "month" not in df.columns:
            df["month"] = pd.to_datetime(df["date"]).dt.month
        df["wet"] = df["month"].isin(WET_MONTHS)
        df["changed"] = np.where(np.isfinite(df.get("delta_3S", np.nan)) & (np.abs(df.get("delta_3S", 0.0)) > 1e-6),
                                 True, False)
        cols += ["h_pred_3S", "delta_3S", "wet", "changed"]
    if "h_pred_Pakse" in df.columns:
        cols += ["h_pred_Pakse", "delta_Pakse"]

    return fig, note, df[cols]

# =============================================================================
# Backtest cache (parquet + metrics json): concurrency-safe on-disk artifacts
# =============================================================================
def _load_or_run_backtest_ytd_cached(
    *,
    year: int,
    k: int,
    station: str,
    model_id: str,
    last_date,   # python date (your water_daily index uses date)
    runner,
    water_daily: pd.Series,
):
    """
    Load YTD backtest from runtime cache (parquet + metrics json) with concurrency safety.

    Cache key dimensions:
      - station, model_id, year, k

    Cache validity:
      - meta["last_date"] == str(last_date)

    Consistency guarantees:
      - Both files written under a lock.
      - Both writes are atomic (temp + replace).
      - Metrics JSON is written AFTER parquet to avoid "new meta + old parquet" mismatch.

    Returns:
      (df, rmse): df is pd.DataFrame; rmse is float|None
    """
    k = int(k)
    year = int(year)

    p = backtest_cache_path(LAYOUT, station, model_id, year, k)          # parquet (Path)
    m = backtest_metrics_cache_path(LAYOUT, station, model_id, year, k)  # json (Path)

    def _read_cached_pair_unlocked():
        """
        Best-effort cache read WITHOUT acquiring a lock.
        Returns:
          (df, rmse) on hit; None on miss/stale/broken.
        """
        if not (p.exists() and m.exists()):
            return None

        try:
            meta = json.loads(m.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"[backtest] cache metrics read failed: {m} -> {repr(e)}")
            return None

        try:
            meta_last = meta.get("last_date")
            if meta_last != str(last_date):
                print(f"[backtest] cache stale: k={k}, year={year} -> meta.last_date={meta_last} != {str(last_date)}")
                return None

            df = pd.read_parquet(p)
            if df is None or len(df) == 0:
                # treat as miss to trigger recompute (defensive)
                print(f"[backtest] cache empty parquet treated as miss: {p}")
                return None

            rmse = meta.get("rmse", None)
            rmse = float(rmse) if rmse is not None else None

            print(f"[backtest] cache hit: k={k}, year={year} -> {p}")
            return df, rmse

        except Exception as e:
            print(f"[backtest] cache parquet read/parse failed: {p} -> {repr(e)}")
            return None

    # Fast path: unlocked read (performance).
    hit = _read_cached_pair_unlocked()
    if hit is not None:
        return hit

    # Slow path: lock + double-check + compute + atomic paired write.
    # Use parquet path as the lock target (one of p/m is enough)
    with lock_for_path(p):
        # double-check inside lock (another request may have filled it while we waited)
        hit = _read_cached_pair_unlocked()
        if hit is not None:
            print(f"[backtest] cache hit(after lock): k={k}, year={year} -> {p}")
            return hit

        # compute
        print(f"[backtest] cache miss -> compute: k={k}, year={year} -> {p}")
        try:
            df, rmse = _backtest_ytd_1day(runner, water_daily, start=f"{year}-01-01", horizon=k)
        except Exception as e:
            print(f"[backtest] compute failed: k={k}, year={year} -> {repr(e)}")
            return None, None

        # write only when we have rows
        if df is not None and len(df) > 0:
            meta = {
                "station": station,
                "model_id": model_id,
                "year": year,
                "k": k,
                "n": int(len(df)),
                "rmse": float(rmse) if rmse is not None else None,
                "last_date": str(last_date),
            }

            try:
                # Defensive mkdir (runtime_files generally ensures this, but keep safe)
                p.parent.mkdir(parents=True, exist_ok=True)
                m.parent.mkdir(parents=True, exist_ok=True)

                # Paired atomic write: parquet first, then metrics
                atomic_write_parquet(p, df, index=False)
                atomic_write_json(m, meta)

                print(f"[backtest] cached: k={k}, year={year} -> {p} (+ {m})")
            except Exception as e:
                # If caching fails, still return computed result
                print(f"[backtest] cache write failed (non-fatal): k={k}, year={year} -> {repr(e)}")

        return df, rmse

# =============================================================================
# Backfill persistence (atomic parquet writes)
# =============================================================================
def _atomic_write_backfill_series(p: Path, s: pd.Series) -> None:
    """
    Atomically persist backfill series to parquet at path p.

    Assumes the backfill parquet schema is:
      - date: datetime64[ns]
      - h: float
    (If your src/backfill.py uses a different column name, adjust here to match.)
    """
    if s is None or len(s) == 0:
        return

    # ensure parent exists
    p.parent.mkdir(parents=True, exist_ok=True)

    df_bf = pd.DataFrame({
        "date": pd.to_datetime(list(s.index)),
        "h": pd.to_numeric(np.asarray(s.values), errors="coerce"),
    }).dropna(subset=["date", "h"])

    # keep deterministic order
    df_bf = df_bf.sort_values("date").reset_index(drop=True)

    atomic_write_parquet(p, df_bf, index=False)

# =============================================================================
# Tab1 callback: Forecast Today → +7 days (optional uncertainty & assists)
# =============================================================================
def ui_predict_today(show_uncertainty=False, src_choice="Historical residuals (fast)", mc_samples=30):
    """
    Tab1 callback: generate the next 7-day absolute water-level forecast (UTC+07).

    Enhancements:
    - Optional uncertainty band:
        * Historical residual band (fast, uses assets/residual_sigma.json)
        * MC Dropout quantiles (slow; cached in-memory per process)
    - Optional upstream assist overlays:
        * Pakse
        * 3S
      Each assist is applied with guardrails: lag alignment, no future-leak, dry-season shrink.

    Returns:
        (fig, note, df_out):
          fig: matplotlib figure (or None on failure)
          note: summary string (data/uncertainty/assist availability)
          df_out: table with date, h_pred, (optional p10/p90), (optional assist columns)
    """
    S = _load_service()
    runner, water_daily = S["runner"], S["water_daily"]
    resid_sigma = S.get("resid_sigma")
    mc_cache = S.get("mc_cache", {})
    pakse_daily = S.get("pakse_daily")
    w3s_daily = S.get("w3s_daily")

    last_date = max(water_daily.index)  # python date, because your index is date
    model_id = S.get("model_id") or Path(_find_ckpt()).name

    # Choose an anchor that guarantees a contiguous, valid SEQ_LENGTH-day window.
    if len(water_daily) < SEQ_LENGTH:
        return None, f"Not enough data for a {SEQ_LENGTH}-day window (currently {len(water_daily)} days).", None
    try:
        anchor = _latest_contiguous_anchor(water_daily, SEQ_LENGTH)
    except Exception as e:
        return None, f"Not enough contiguous data: {e}", None

    # Build model input window.
    try:
        Xn, fut_dates = _build_window_Xn(runner, water_daily, pd.Timestamp(anchor))
    except Exception as e:
        return None, f"Failed to build input window: {e}", None

    # central (deterministic) prediction
    t0 = time.perf_counter()
    y_abs = _predict_7_abs(runner, Xn, fut_dates, training=False)  # [7]
    latency_ms = (time.perf_counter() - t0) * 1000

    # -------------------------------------------------------------------------
    # Uncertainty band
    # -------------------------------------------------------------------------
    lo = hi = None
    band_note = ""
    if show_uncertainty:
        if src_choice.startswith("Historical residuals"):
            if resid_sigma and "by_horizon" in resid_sigma:
                sigma = np.array(resid_sigma["by_horizon"], dtype=np.float32)  # [7]
                lo = y_abs - 1.96 * sigma
                hi = y_abs + 1.96 * sigma
                band_note = f"Historical residual band ±1.96σ (n={resid_sigma.get('n', '?')})"
            else:
                # Fallback to MC if residual artifact is missing.
                src_choice = "MC Dropout (slow)"
                band_note = "residual_sigma.json not found; fell back to MC Dropout."

        if src_choice.startswith("MC Dropout"):
            key = (str(anchor), int(mc_samples))
            if key in mc_cache:
                lo, hi = mc_cache[key]
                band_note = f"MC Dropout p10–90 (cache hit, N={mc_samples})"
            else:
                N = int(mc_samples)
                Ys = [_predict_7_abs(runner, Xn, fut_dates, training=True) for _ in range(N)]
                Ys = np.stack(Ys, axis=0)  # [N,7]
                lo, hi = np.percentile(Ys, [10, 90], axis=0)
                mc_cache[key] = (lo, hi)
                band_note = f"MC Dropout p10–90 (N={mc_samples})"

    # -------------------------------------------------------------------------
    # Plot: baseline + optional overlays
    # -------------------------------------------------------------------------
    fig = plt.figure(figsize=(9.5, 4.2))
    plt.plot(fut_dates, y_abs, label="FNO (mean)", linewidth=2)

    # ---- Pakse assist overlay (with cache + guardrails) ----
    pk_note = ""
    params_pk = _load_or_fit_assist_params(
        STATION_CODE,
        "Pakse",
        pakse_daily,
        fit_fn=lambda: _fit_pakse_params_for_tab1(runner, water_daily, pakse_daily, horizon_for_fit=1),
        model_id=model_id,
        last_date=last_date,
    )

    if params_pk:
        y_pk, used_pk, avail_pk, k_pk = _apply_upstream_correction_future(
            y_abs, fut_dates, pakse_daily, params_pk, shrink_dry=DRY_SHRINK, allow_interp=True
        )
        y_pk_plot = y_pk.copy()
        y_pk_plot[~used_pk] = np.nan
        if avail_pk > 0:
            plt.plot(fut_dates, y_pk_plot, label=f"FNO + Pakse (k={k_pk}; avail={avail_pk}/{PRED_LENGTH})", linewidth=2)
            try:
                plt.plot(np.array(fut_dates, dtype="datetime64[ns]")[used_pk], y_pk[used_pk], "o", markersize=3)
            except Exception:
                pass
            pk_note = (
                f" | Pakse assist: k={k_pk}, avail={avail_pk}/{PRED_LENGTH}, "
                f"source=recent\\~29d incl.today; dry-shrink λ={DRY_SHRINK}"
            )
        else:
            pk_note = f" | Pakse not available for this window (k={k_pk}); source=recent\\~29d incl.today"
    else:
        pk_note = " | Pakse assist unavailable (insufficient data/fit)"

    # ---- 3S assist overlay (with cache + guardrails) ----
    s3_note = ""
    params_3s = _load_or_fit_assist_params(
        STATION_CODE,
        "3S",
        w3s_daily,
        fit_fn=lambda: _fit_w3s_params_for_tab1(runner, water_daily, w3s_daily, horizon_for_fit=1),
        model_id=model_id,
        last_date=last_date,
    )

    if params_3s:
        y_3s, used_3s, avail_3s, k_3s = _apply_upstream_correction_future(
            y_abs, fut_dates, w3s_daily, params_3s, shrink_dry=DRY_SHRINK, allow_interp=True
        )
        y_3s_plot = y_3s.copy()
        y_3s_plot[~used_3s] = np.nan
        if avail_3s > 0:
            plt.plot(fut_dates, y_3s_plot, label=f"FNO + 3S (k={k_3s}; avail={avail_3s}/{PRED_LENGTH})", linewidth=2)
            try:
                plt.plot(np.array(fut_dates, dtype="datetime64[ns]")[used_3s], y_3s[used_3s], "o", markersize=3)
            except Exception:
                pass
            s3_note = (
                f" | 3S assist: k={k_3s}, avail={avail_3s}/{PRED_LENGTH}, "
                f"source=CSV⊕recent\\~29d incl.today; dry-shrink λ={DRY_SHRINK}"
            )
        else:
            s3_note = f" | 3S not available for this window (k={k_3s}); source=CSV⊕recent\\~29d incl.today"
    else:
        s3_note = " | 3S assist unavailable (insufficient data/fit)"

    if lo is not None:
        plt.fill_between(fut_dates, lo, hi, alpha=0.18, label=band_note or "Uncertainty band")
    plt.title("Next 7-day absolute water level (UTC+07)")
    plt.xlabel("Date"); plt.ylabel("Water Level (m)")
    plt.xticks(rotation=20); plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout()

    # -------------------------------------------------------------------------
    # Output table
    # -------------------------------------------------------------------------
    df_out = pd.DataFrame({"date": pd.to_datetime(fut_dates).date, "h_pred": y_abs})
    if lo is not None:
        df_out["p10"] = lo; df_out["p90"] = hi

    # output the Pakse-corrected column and the availability flag
    if params_pk:
        # If params exist, the above y_pk/used_pk have been computed (otherwise pk_note will indicate unavailable/not fitted)
        try:
            df_out["h_pred_Pakse"] = y_pk
            df_out["pakse_used"]   = used_pk
        except Exception:
            # If y_pk/used_pk are absent (e.g., fitting failed), skip
            pass

    # 3S-corrected column and availability flag
    if params_3s:
        try:
            df_out["h_pred_3S"] = y_3s
            df_out["s3_used"] = used_3s
        except Exception:
            pass

    note = "Next 7-day absolute water level (UTC+07)"
    if band_note:
        note += f"; Uncertainty: {band_note}"
    note += (s3_note or "")
    note += (pk_note or "")
    return fig, note, df_out

# =============================================================================
# Phase-report helpers (assets/phase_report.json)
# =============================================================================
def _load_phase_json_or_fallback():
    """
    Load the phase-alignment report from assets, if available.

    Returns:
        dict | None: Parsed JSON payload, or None if the artifact is missing.
    """
    if os.path.exists(PHASE_JSON):
        with open(PHASE_JSON, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

def ui_phase_table(scope):
    """
    Tab2 callback: render a one-row summary table from phase_report.json.

    Args:
        scope: One of {"Merged","Dry","Wet"} selecting which evaluation window to display.

    Returns:
        pd.DataFrame: A one-row table (or a one-row message when the artifact is missing).
    """
    mapping = {"Merged":"all", "Dry":"dry", "Wet":"wet"}
    key = mapping.get(scope, "all")
    rep = _load_phase_json_or_fallback()
    if rep is None:
        msg = "Missing assets/phase_report.json. Please run: python -m scripts.make_phase_report"
        return pd.DataFrame({"message":[msg]})
    row = rep["test_applied"][key]  # {k, rmse_before, rmse_after, gain}
    df = pd.DataFrame([{
        "Window": scope,
        "k* (days)": row["k"],
        "RMSE (raw)": round(row["rmse_before"], 3),
        "RMSE (aligned)": round(row["rmse_after"], 3),
        "ΔRMSE": round(row["gain"], 3),
    }])
    return df

# =============================================================================
# Tab2 callbacks: windowed comparisons (FNO vs FNO + upstream assist)
# =============================================================================
def ui_compare_fno_vs_3s_window(horizon=1):
    """
    Compare baseline FNO vs FNO + 3S assist on the intersected window where 3S is usable.

    Policy:
    - Fit assist params on the same YTD backtest rows.
    - Evaluate only on dates where the lagged upstream features exist (no extrapolation).

    Returns:
        pd.DataFrame: One-row metrics table, or a message table if comparison cannot run.
    """
    S = _load_service()
    runner, water_daily = S["runner"], S["water_daily"]
    w3s_daily = S.get("w3s_daily")

    year = 2025
    k_h = int(horizon)
    last_date = max(water_daily.index)
    model_id = S.get("model_id") or Path(_find_ckpt()).name

    df, _ = _load_or_run_backtest_ytd_cached(
        year=year,
        k=k_h,
        station=STATION_CODE,
        model_id=model_id,
        last_date=last_date,
        runner=runner,
        water_daily=water_daily,
    )

    if df is None or len(df) == 0:
        return pd.DataFrame({"message": [f"Not enough data (h={horizon})."]})
    if w3s_daily is None or len(w3s_daily) == 0:
        return pd.DataFrame({"message": ["3S daily series (014500) is empty."]})

    params = _fit_3s_residual_model(df, w3s_daily, k_grid=(0, 1, 2, 3))
    if not params:
        return pd.DataFrame({"message": ["Not enough samples to fit 3S assist parameters."]})

    y_corr, _ = _apply_3s_correction(df, w3s_daily, params)

    k = int(params["k"])
    dates = pd.to_datetime(df["date"]).dt.date.values
    lag_dates = np.array([d - pd.Timedelta(days=k) for d in dates], dtype="object")
    has_3s = np.array([(d in w3s_daily.index) and pd.notna(w3s_daily.get(d)) for d in lag_dates], dtype=bool)

    h_true = df["h_true"].values.astype(float)
    h_pred = df["h_pred"].values.astype(float)
    mask = has_3s & np.isfinite(h_true) & np.isfinite(h_pred) & np.isfinite(y_corr)
    if mask.sum() == 0:
        return pd.DataFrame({"message": ["No overlapping dates where 3S (with lag k) is available."]})

    idx = np.where(mask)[0]; d_sub = dates[idx]
    rmse_fno = float(np.sqrt(np.mean((h_pred[idx] - h_true[idx]) ** 2)))
    rmse_3s  = float(np.sqrt(np.mean((y_corr[idx]  - h_true[idx]) ** 2)))
    mae_fno  = float(np.mean(np.abs(h_pred[idx] - h_true[idx])))
    mae_3s   = float(np.mean(np.abs(y_corr[idx]  - h_true[idx])))

    return pd.DataFrame([{
        "Window":        f"3S-available (with lag k, h={horizon})",
        "k (days)":      k,
        "N":             int(len(idx)),
        "From":          str(d_sub.min()),
        "To":            str(d_sub.max()),
        "RMSE (FNO)":    round(rmse_fno, 3),
        "RMSE (FNO+3S)": round(rmse_3s,  3),
        "ΔRMSE":         round(rmse_3s - rmse_fno, 3),
        "MAE (FNO)":     round(mae_fno,  3),
        "MAE (FNO+3S)":  round(mae_3s,   3),
        "ΔMAE":          round(mae_3s - mae_fno, 3),
    }])

def ui_compare_fno_vs_pakse_window(horizon=1):
    """
    Compare baseline FNO vs FNO + Pakse assist on the intersected window where Pakse is usable.

    Returns:
        pd.DataFrame: One-row metrics table, or a message table if comparison cannot run.
    """
    S = _load_service()
    runner, water_daily = S["runner"], S["water_daily"]
    pakse_daily = S.get("pakse_daily")

    year = 2025
    k_h = int(horizon)
    last_date = max(water_daily.index)
    model_id = S.get("model_id") or Path(_find_ckpt()).name

    df, _ = _load_or_run_backtest_ytd_cached(
        year=year,
        k=k_h,
        station=STATION_CODE,
        model_id=model_id,
        last_date=last_date,
        runner=runner,
        water_daily=water_daily,
    )

    if df is None or len(df) == 0:
        return pd.DataFrame({"message": [f"Not enough data (h={horizon})."]})
    if pakse_daily is None or len(pakse_daily) == 0:
        return pd.DataFrame({"message": ["Pakse daily series (013901) is empty."]})

    params_pk = _fit_3s_residual_model(df, pakse_daily, k_grid=(0, 1, 2, 3))
    if not params_pk:
        return pd.DataFrame({"message": ["Not enough samples to fit Pakse assist parameters."]})

    y_corr_pk, _ = _apply_3s_correction(df, pakse_daily, params_pk)

    k = int(params_pk["k"])
    dates = pd.to_datetime(df["date"]).dt.date.values
    lag_dates = np.array([d - pd.Timedelta(days=k) for d in dates], dtype="object")
    has_pk = np.array([(d in pakse_daily.index) and pd.notna(pakse_daily.get(d)) for d in lag_dates], dtype=bool)

    h_true = df["h_true"].values.astype(float)
    h_pred = df["h_pred"].values.astype(float)
    mask = has_pk & np.isfinite(h_true) & np.isfinite(h_pred) & np.isfinite(y_corr_pk)
    if mask.sum() == 0:
        return pd.DataFrame({"message": ["No overlapping dates where Pakse (with lag k) is available."]})

    idx = np.where(mask)[0]; d_sub = dates[idx]
    rmse_fno = float(np.sqrt(np.mean((h_pred[idx] - h_true[idx]) ** 2)))
    rmse_pk  = float(np.sqrt(np.mean((y_corr_pk[idx] - h_true[idx]) ** 2)))
    mae_fno  = float(np.mean(np.abs(h_pred[idx] - h_true[idx])))
    mae_pk   = float(np.mean(np.abs(y_corr_pk[idx] - h_true[idx])))

    return pd.DataFrame([{
        "Window":            f"Pakse-available (with lag k, h={horizon})",
        "k (days)":          k,
        "N":                 int(len(idx)),
        "From":              str(d_sub.min()),
        "To":                str(d_sub.max()),
        "RMSE (FNO)":        round(rmse_fno, 3),
        "RMSE (FNO+Pakse)":  round(rmse_pk,  3),
        "ΔRMSE":             round(rmse_pk - rmse_fno, 3),
        "MAE (FNO)":         round(mae_fno,  3),
        "MAE (FNO+Pakse)":   round(mae_pk,   3),
        "ΔMAE":              round(mae_pk - mae_fno, 3),
    }])

# =============================================================================
# Service lifecycle (manual reload)
# =============================================================================
def ui_reload_service():
    """
    Gradio callback: force reload model & data into the process-level cache.

    Returns:
        str: Status string including reload timestamp and latest available date.
    """
    t0 = time.perf_counter()

    # force reload
    S = _load_service(force_reload=True)

    water_daily = S.get("water_daily")
    try:
        last_day = max(water_daily.index) if water_daily is not None and len(water_daily) > 0 else None
    except Exception:
        last_day = None

    now = pd.Timestamp.now(tz=ZoneInfo("Asia/Bangkok"))
    dt_str = now.strftime("%Y-%m-%d %H:%M")

    msg = f"Reloaded at {dt_str} (UTC+07); latest water_daily date = {last_day}"
    elapsed = time.perf_counter() - t0
    msg += f" | reload took {elapsed:.2f} s"

    return msg

# =============================================================================
# Gradio app construction (layout + wiring)
# =============================================================================
def build_app():
    """
    Construct the Gradio Blocks UI and wire callbacks.

    Design:
    - Layout is declarative (components created in one place).
    - Business logic lives in ui_* callbacks.
    - Heavy loading happens inside `_load_service()` (explicit warm-up in __main__).

    Returns:
        gr.Blocks: Fully wired application.
    """
    with gr.Blocks(title="Mekong FNO Demo") as demo:
        gr.Markdown(
            "### Mekong Water Level Forecast (Stung Treng) — FNO\n"
            "- Tab1: **Forecast Today → +7 days** (optional uncertainty)\n"
            "- Tab2: **ΔRMSE alignment evaluation** (reads `assets/phase_report.json`)"
        )

        # ---------------------------------------------------------------------
        # Global controls (shared by both tabs)
        # ---------------------------------------------------------------------
        with gr.Row():
            reload_btn = gr.Button("Reload data/model", variant="secondary")
            reload_note = gr.Markdown()
        reload_btn.click(fn=ui_reload_service, inputs=None, outputs=reload_note)

        # ---------------------------------------------------------------------
        # Tabs
        # ---------------------------------------------------------------------
        with gr.Tabs():
            # =================================================================
            # Tab1: Forecast (Today → +7 days)
            # =================================================================
            with gr.Tab("Forecast (Today → +7 days)"):
                with gr.Row():
                    btn = gr.Button("Forecast +7 Days (UTC+07)", variant="primary")
                    ck = gr.Checkbox(value=False, label="Show uncertainty (Residuals/MC)")
                    src = gr.Radio(
                        choices=["Historical residuals (fast)", "MC Dropout (slow)"],
                        value="Historical residuals (fast)",
                        label="Uncertainty source",
                    )
                    samp = gr.Slider(10, 100, value=30, step=5, label="MC samples", interactive=True)

                out_plot = gr.Plot()
                out_note = gr.Markdown()
                out_df = gr.Dataframe(headers=["date", "h_pred", "p10", "p90"], interactive=False)

                # Dataflow: (ck, src, samp) -> ui_predict_today -> (plot, note, table)
                btn.click(fn=ui_predict_today, inputs=[ck, src, samp], outputs=[out_plot, out_note, out_df])

            # =================================================================
            # Tab2: Evaluation (2025 YTD & ΔRMSE)
            # =================================================================
            with gr.Tab("Evaluation (2025 YTD & ΔRMSE)"):
                # shared horizon selector for backtest/compare
                with gr.Row():
                    h_sel = gr.Slider(1, 7, value=1, step=1, label="Backtest horizon (days ahead)", interactive=True)

                # ----------------- YTD backtest -----------------
                with gr.Row():
                    btn_bt = gr.Button("Run 2025 YTD backtest (k-day ahead)", variant="primary")
                ytd_plot = gr.Plot()
                ytd_note = gr.Markdown()
                ytd_df = gr.Dataframe(interactive=False)

                # Dataflow: h_sel -> ui_eval_ytd -> (plot, note, df)
                bt_evt = btn_bt.click(fn=ui_eval_ytd, inputs=h_sel, outputs=[ytd_plot, ytd_note, ytd_df])

                # ----------------- Comparisons: 3S -----------------
                gr.Markdown("### FNO vs FNO + 3S (Only where 3S is available)")
                with gr.Row():
                    btn_cmp = gr.Button("Compare on 3S-available dates (RMSE & MAE)", variant="secondary")
                cmp_tbl = gr.Dataframe(interactive=False)

                # manual refresh
                btn_cmp.click(fn=ui_compare_fno_vs_3s_window, inputs=h_sel, outputs=cmp_tbl)
                # automatically refresh once after the backtest completes
                bt_evt.then(fn=ui_compare_fno_vs_3s_window, inputs=h_sel, outputs=cmp_tbl)

                # ----------------- Comparisons: Pakse -----------------
                gr.Markdown("### FNO vs FNO + Pakse (Only where Pakse is available)")
                with gr.Row():
                    btn_cmp_pk = gr.Button("Compare on Pakse-available dates (RMSE & MAE)", variant="secondary")
                pk_tbl = gr.Dataframe(interactive=False)

                # manual refresh
                btn_cmp_pk.click(fn=ui_compare_fno_vs_pakse_window, inputs=h_sel, outputs=pk_tbl)
                # automatically refresh once after the backtest completes
                bt_evt.then(fn=ui_compare_fno_vs_pakse_window, inputs=h_sel, outputs=pk_tbl)

                gr.Markdown("---")

                # ----------------- Phase report table -----------------
                scope = gr.Radio(choices=["Merged", "Dry", "Wet"], value="Merged", label="Select window")
                tbl = gr.Dataframe(interactive=False)
                scope.change(fn=ui_phase_table, inputs=scope, outputs=tbl)
                gr.Markdown(
                    "> Note: scan the optimal phase shift k* on 2023, then fix it on the corresponding 2024 windows. "
                    "Shows RMSE before/after alignment and ΔRMSE."
                )
    return demo

# =============================================================================
# Entrypoint (__main__): warmup + launch
# =============================================================================
if __name__ == "__main__":
    # Runtime layout visibility (useful in HF logs / debugging storage behavior).
    print(f"[runtime] root={LAYOUT.root}")
    print(f"[runtime] cache={LAYOUT.cache}")
    print(f"[runtime] artifacts={LAYOUT.artifacts}")

    # Warm-up: preload model + data to reduce first-request latency.
    _load_service()

    app = build_app()
    app.launch(server_name="0.0.0.0",
               server_port=7860,
               theme=gr.themes.Soft(),
               )